{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd686b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45432e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(\"iris\", \"iris.data\"), header=None)\n",
    "\n",
    "classes = np.unique(df[4].values)\n",
    "df[4] = df[4].map({'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6267ce89",
   "metadata": {},
   "source": [
    "### Divide the dataset into training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc42f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_iris_data(df, training_split=0.5, random_state=None):\n",
    "    setosa = df[df[4] == 0]\n",
    "    versicolor = df[df[4] == 1] \n",
    "    virginica = df[df[4] == 2]\n",
    "\n",
    "    # Randomly split each class into the training set\n",
    "    train_set = pd.concat([setosa.sample(frac=training_split, random_state=random_state),\n",
    "                            versicolor.sample(frac=training_split, random_state=random_state),\n",
    "                            virginica.sample(frac=training_split, random_state=random_state)])\n",
    "\n",
    "    # The remaining data composes the test set\n",
    "    test_set = df.drop(train_set.index)\n",
    "\n",
    "    # Shuffle both datasets\n",
    "    train_set = train_set.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    test_set = test_set.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    \n",
    "    # Make training targets and features\n",
    "    train_features = train_set.drop(columns=[4]).values\n",
    "    train_targets = train_set[4].values\n",
    "    # Make test targets and features\n",
    "    test_features = test_set.drop(columns=[4]).values\n",
    "    test_targets = test_set[4].values\n",
    "\n",
    "    return train_features, train_targets, test_features, test_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30c1334",
   "metadata": {},
   "source": [
    "### Fisher's linear discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c59f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class FisherLDA:\n",
    "    \"\"\"\n",
    "    Fisher's LDA finds a projection direction w that maximizes the Fisher criterion:\n",
    "    J(w) = (w^T * Sb * w) / (w^T * Sw * w)\n",
    "    \n",
    "    Where:\n",
    "    - Sb is the between-class scatter matrix\n",
    "    - Sw is the within-class scatter matrix\n",
    "    - w is the projection vector we want to find\n",
    "    \n",
    "    The optimal w is found by solving the generalized eigenvalue problem:\n",
    "    Sb * w = gamma * Sw * w\n",
    "    Which is equivalent to: Sw^(-1) * Sb * w = gamma * w\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_components=2):\n",
    "        self.n_components = n_components\n",
    "        self.W = None  \n",
    "        self.class_means = None\n",
    "        self.overall_mean = None\n",
    "        self.Sw = None  \n",
    "        self.Sb = None  \n",
    "        \n",
    "    def _calculate_class_means(self, X, y):\n",
    "        classes = np.unique(y)\n",
    "        class_means = {}\n",
    "        for cls in classes:\n",
    "            class_means[cls] = np.mean(X[y == cls], axis=0)\n",
    "        return class_means\n",
    "    \n",
    "    def _calculate_within_class_scatter(self, X, y):\n",
    "        classes = np.unique(y)\n",
    "        n_features = X.shape[1]\n",
    "        Sw = np.zeros((n_features, n_features))\n",
    "        \n",
    "        for cls in classes:\n",
    "            class_data = X[y == cls]\n",
    "            class_mean = self.class_means[cls]\n",
    "            \n",
    "            # Calculate scatter for this class: Σ(x - μ_i)(x - μ_i)^T\n",
    "            class_scatter = np.zeros((n_features, n_features))\n",
    "            for sample in class_data:\n",
    "                diff = (sample - class_mean).reshape(-1, 1)\n",
    "                class_scatter += diff @ diff.T\n",
    "            \n",
    "            Sw += class_scatter\n",
    "            \n",
    "        return Sw\n",
    "    \n",
    "    def _calculate_between_class_scatter(self, X, y):\n",
    "        classes = np.unique(y)\n",
    "        n_features = X.shape[1]\n",
    "        Sb = np.zeros((n_features, n_features))\n",
    "        \n",
    "        for cls in classes:\n",
    "            n_samples = np.sum(y == cls)\n",
    "            class_mean = self.class_means[cls]\n",
    "            \n",
    "            # Calculate (μ_i - μ)(μ_i - μ)^T\n",
    "            diff = (class_mean - self.overall_mean).reshape(-1, 1)\n",
    "            Sb += n_samples * (diff @ diff.T)\n",
    "            \n",
    "        return Sb\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\" \n",
    "        Optimizes \n",
    "        max J(w) = (w^T * Sb * w) / (w^T * Sw * w) \n",
    "        by finding eigenvectors of Sw^(-1) * Sb\n",
    "        \"\"\"        \n",
    "        self.overall_mean = np.mean(X, axis=0)\n",
    "        self.class_means = self._calculate_class_means(X, y)\n",
    "        \n",
    "        # Calculate scatter matrices\n",
    "        self.Sw = self._calculate_within_class_scatter(X, y)\n",
    "        self.Sb = self._calculate_between_class_scatter(X, y)\n",
    "        \n",
    "        # Solve the generalized eigenvalue problem: Sw^(-1) * Sb * w = gamma * w\n",
    "        Sw_inv = np.linalg.pinv(self.Sw)\n",
    "        eigenvals, eigenvecs = np.linalg.eig(Sw_inv @ self.Sb)\n",
    "        \n",
    "        idx = np.argsort(eigenvals.real)[::-1]\n",
    "        eigenvals = eigenvals[idx]\n",
    "        eigenvecs = eigenvecs[:, idx]\n",
    "        \n",
    "        # Select the top n_components eigenvectors\n",
    "        self.W = eigenvecs[:, :self.n_components].real\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X @ self.W\n",
    "    \n",
    "    def fit_transform(self, X, y):\n",
    "        return self.fit(X, y).transform(X)\n",
    "\n",
    "def one_vs_rest_lda(X_train, y_train, X_test, y_test, plot=True):\n",
    "    \"\"\"\n",
    "    Perform one-vs-rest Fisher's LDA for each class\n",
    "    \n",
    "    For each class i:\n",
    "    1. Create binary problem: class i vs all other classes\n",
    "    2. Apply Fisher's LDA to find optimal projection using training data\n",
    "    3. Evaluate classification performance on test data\n",
    "    \"\"\"\n",
    "    classes = np.unique(y_train)\n",
    "    results = {}\n",
    "    \n",
    "    for target_class in classes:        \n",
    "        # Create binary labels for train and test: 1 for target class, 0 for others\n",
    "        y_train_binary = (y_train == target_class).astype(int)\n",
    "        y_test_binary = (y_test == target_class).astype(int)\n",
    "        \n",
    "        # Apply Fisher's LDA with n_components=1 on training data\n",
    "        lda = FisherLDA(n_components=1)\n",
    "        X_train_proj = lda.fit_transform(X_train, y_train_binary)\n",
    "        \n",
    "        # Transform test data using fitted LDA\n",
    "        X_test_proj = lda.transform(X_test)\n",
    "        \n",
    "        # Calculate Fisher's criterion value\n",
    "        fisher_criterion = (lda.W.T @ lda.Sb @ lda.W) / (lda.W.T @ lda.Sw @ lda.W)\n",
    "        \n",
    "        # Calculate threshold using midpoint of projected class means\n",
    "        mean_target_proj = lda.class_means[1] @ lda.W.flatten()  \n",
    "        mean_rest_proj = lda.class_means[0] @ lda.W.flatten()    \n",
    "        threshold = (mean_target_proj + mean_rest_proj) / 2\n",
    "        \n",
    "        target_above_threshold = mean_target_proj > threshold        \n",
    "        if target_above_threshold:\n",
    "            # Standard case: target class projects to values > threshold\n",
    "            predictions_one = X_test_proj[y_test_binary == 1].flatten() > threshold\n",
    "            predictions_rest = X_test_proj[y_test_binary == 0].flatten() <= threshold\n",
    "        else:\n",
    "            # Flipped case: target class projects to values < threshold\n",
    "            predictions_one = X_test_proj[y_test_binary == 1].flatten() < threshold\n",
    "            predictions_rest = X_test_proj[y_test_binary == 0].flatten() >= threshold\n",
    "\n",
    "        # Calculate accuracy\n",
    "        correct_predictions = np.sum(predictions_one) + np.sum(predictions_rest)\n",
    "        total_samples = len(predictions_one) + len(predictions_rest)\n",
    "        accuracy = correct_predictions / total_samples\n",
    "\n",
    "        results[target_class] = {\n",
    "            'accuracy': accuracy,\n",
    "        }\n",
    "        \n",
    "        # Plot results using test data\n",
    "        if plot:\n",
    "            plt.figure(figsize=(12, 5))\n",
    "            \n",
    "            # Plot 1: Original test data\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plot_df_orig = pd.DataFrame({\n",
    "                'Feature 1': X_test[:, 0],\n",
    "                'Feature 2': X_test[:, 1], \n",
    "                'Class': ['Target Class' if label == 1 else 'Other Classes' for label in y_test_binary]\n",
    "            })\n",
    "            sns.scatterplot(data=plot_df_orig, x='Feature 1', y='Feature 2', hue='Class', \n",
    "                            palette={'Other Classes': 'red', 'Target Class': 'blue'}, alpha=0.7)\n",
    "            plt.title(f'Test Data - Class {target_class} vs Rest')\n",
    "            # Plot 2: LDA projections of test data\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plot_df_lda = pd.DataFrame({\n",
    "                'LDA Projection': X_test_proj.flatten(),\n",
    "                'Class': ['Target Class' if label == 1 else 'Other Classes' for label in y_test_binary],\n",
    "                'Index': range(len(X_test_proj.flatten()))\n",
    "            })\n",
    "            \n",
    "            sns.scatterplot(data=plot_df_lda, x='Index', y='LDA Projection', hue='Class', \n",
    "                            palette={'Other Classes': 'red', 'Target Class': 'blue'}, \n",
    "                            alpha=0.7, s=60)\n",
    "            \n",
    "            # Add decision boundary\n",
    "            plt.axhline(threshold, color='green', linestyle='--', linewidth=2, label='Decision boundary')\n",
    "            plt.title(f'LDA Projection - Class {target_class} vs Rest\\nTest Accuracy: {accuracy:.2%}')\n",
    "            plt.xlabel('Sample Index')\n",
    "            plt.ylabel('LDA Projection')\n",
    "            plt.legend(loc='upper right')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.show()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be50101a",
   "metadata": {},
   "source": [
    "### One vs. Rest Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b87461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare the data\n",
    "X_train, y_train, X_test, y_test = split_iris_data(df, training_split=0.5)\n",
    "\n",
    "# Perform one-vs-rest LDA\n",
    "results = one_vs_rest_lda(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Summary\n",
    "for cls, result in results.items():\n",
    "    print(f\"Class {cls}: Accuracy = {result['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabcae02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Fisher's LDA One-vs-Rest Implementation (20 runs) --\n",
    "all_runs_results = []\n",
    "\n",
    "for run in range(20):  \n",
    "    # Split the data for this run (no random_state for variability)\n",
    "    X_train, y_train, X_test, y_test = split_iris_data(df, training_split=0.5)\n",
    "    \n",
    "    run_results = {}\n",
    "    \n",
    "    # Loop through each class to perform one-vs-rest classification\n",
    "    for i, class_name in enumerate(classes):\n",
    "        \n",
    "        # Prepare Data for One-vs-Rest \n",
    "        y_train_ovr = np.where(y_train == i, 1, 0)\n",
    "        y_test_ovr = np.where(y_test == i, 1, 0)\n",
    "\n",
    "        X_train_one = X_train[y_train_ovr == 1]\n",
    "        X_train_rest = X_train[y_train_ovr == 0]\n",
    "        \n",
    "        # Calculate Mean Vectors\n",
    "        mean_one = np.mean(X_train_one, axis=0)\n",
    "        mean_rest = np.mean(X_train_rest, axis=0)\n",
    "\n",
    "        # Calculate Scatter matrices and the projection vector w\n",
    "        s_one = (X_train_one - mean_one).T @ (X_train_one - mean_one)\n",
    "        s_rest = (X_train_rest - mean_rest).T @ (X_train_rest - mean_rest)\n",
    "        Sw = s_one + s_rest\n",
    "        Sw_inv = np.linalg.inv(Sw)\n",
    "        w = Sw_inv @ (mean_one - mean_rest)\n",
    "\n",
    "        # Project Data and Find Threshold \n",
    "        X_test_one = X_test[y_test_ovr == 1]\n",
    "        X_test_rest = X_test[y_test_ovr == 0]\n",
    "\n",
    "        projected_one = X_test_one @ w\n",
    "        projected_rest = X_test_rest @ w\n",
    "\n",
    "        # A simple threshold is the midpoint of the projected means\n",
    "        threshold = (mean_one @ w + mean_rest @ w) / 2\n",
    "        predictions_one = projected_one > threshold\n",
    "        predictions_rest = projected_rest > threshold\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        correct_predictions = np.sum(predictions_one == 1) + np.sum(predictions_rest == 0)\n",
    "        total_samples = len(projected_one) + len(projected_rest)\n",
    "        accuracy = correct_predictions / total_samples\n",
    "        \n",
    "        run_results[class_name] = accuracy\n",
    "    \n",
    "        all_runs_results.append(run_results)\n",
    "\n",
    "for i, class_name in enumerate(classes):\n",
    "    accuracies = [run[class_name] for run in all_runs_results]\n",
    "    mean_acc = np.mean(accuracies)\n",
    "    var_acc = np.var(accuracies)\n",
    "    min_acc = np.min(accuracies)\n",
    "    max_acc = np.max(accuracies)\n",
    "    \n",
    "    print(f\"\\n{class_name} vs. Rest:\")\n",
    "    print(f\"  Mean Accuracy: {mean_acc:.6f}\")\n",
    "    print(f\"  Variance: {var_acc:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
